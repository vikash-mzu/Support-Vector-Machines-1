{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b8c6e-2df4-4795-8346-3bea66014b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the Mathematical Formula for a Linear SVM?\n",
    "A Support Vector Machine (SVM) aims to find the optimal hyperplane that separates classes in the feature space. For a linear SVM, the decision function can be represented as:\n",
    "f(x)=wTx+bf(x) = \\mathbf{w}^T \\mathbf{x} + bf(x)=wTx+b\n",
    "where:\n",
    "•\tw\\mathbf{w}w is the weight vector (normal to the hyperplane),\n",
    "•\tx\\mathbf{x}x is the feature vector,\n",
    "•\tbbb is the bias term.\n",
    "The hyperplane is defined as:\n",
    "wTx+b=0\\mathbf{w}^T \\mathbf{x} + b = 0wTx+b=0\n",
    "Q2. What is the Objective Function of a Linear SVM?\n",
    "The objective function of a linear SVM is to maximize the margin between the classes while minimizing the classification error. The margin is defined as the distance between the hyperplane and the nearest data points from either class (the support vectors). The optimization problem can be formulated as:\n",
    "min⁡w,b12∥w∥2\\min_{\\mathbf{w}, b} \\frac{1}{2} \\| \\mathbf{w} \\|^2minw,b21∥w∥2\n",
    "subject to:\n",
    "yi(wTxi+b)≥1y_i (\\mathbf{w}^T \\mathbf{x_i} + b) \\geq 1yi(wTxi+b)≥1\n",
    "where yiy_iyi are the class labels and xi\\mathbf{x_i}xi are the feature vectors. This formulation is known as the hard-margin SVM. For soft-margin SVMs, slack variables ξi\\xi_iξi are introduced to handle non-linearly separable data, and the objective function becomes:\n",
    "min⁡w,b,ξ12∥w∥2+C∑i=1Nξi\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2} \\| \\mathbf{w} \\|^2 + C \\sum_{i=1}^{N} \\xi_iminw,b,ξ21∥w∥2+C∑i=1Nξi\n",
    "where CCC is the regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error.\n",
    "Q3. What is the Kernel Trick in SVM?\n",
    "The kernel trick is a method used in SVMs to handle non-linearly separable data by mapping the input features into a higher-dimensional space where a linear separation is possible. Instead of explicitly computing the coordinates in the higher-dimensional space, the kernel trick uses a kernel function K(xi,xj)K(\\mathbf{x}_i, \\mathbf{x}_j)K(xi,xj) to compute the inner product of data points in this space.\n",
    "Common kernel functions include:\n",
    "•\tLinear Kernel: K(xi,xj)=xiTxjK(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_jK(xi,xj)=xiTxj\n",
    "•\tPolynomial Kernel: K(xi,xj)=(αxiTxj+c)dK(\\mathbf{x}_i, \\mathbf{x}_j) = (\\alpha \\mathbf{x}_i^T \\mathbf{x}_j + c)^dK(xi,xj)=(αxiTxj+c)d\n",
    "•\tRadial Basis Function (RBF) Kernel: K(xi,xj)=exp⁡(−γ∥xi−xj∥2)K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\| \\mathbf{x}_i - \\mathbf{x}_j \\|^2)K(xi,xj)=exp(−γ∥xi−xj∥2)\n",
    "Q4. What is the Role of Support Vectors in SVM? Explain with Example\n",
    "Support vectors are the data points that lie closest to the decision boundary (hyperplane) and are crucial in defining the position and orientation of the hyperplane. They are the most informative data points for the model, as they lie on the margin boundaries.\n",
    "Example: Consider a 2D dataset where you want to classify data points into two classes. The support vectors are the data points that are on the edge of the margin on either side of the hyperplane. These points are used to maximize the margin between the two classes. The hyperplane is determined based on these support vectors, and removing or altering them can change the position of the hyperplane.\n",
    "Q5. Illustrate with Examples and Graphs of Hyperplane, Marginal Plane, Soft Margin, and Hard Margin in SVM\n",
    "Hard Margin SVM:\n",
    "•\tThe hyperplane is positioned such that the margin between the classes is maximized, and there are no misclassified points.\n",
    "•\tThe constraints are strictly satisfied.\n",
    "Soft Margin SVM:\n",
    "•\tAllows some misclassification (slack variables) to handle non-linearly separable data.\n",
    "•\tA balance is struck between maximizing the margin and minimizing misclassification errors.\n",
    "Visual Example:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Use only the first two features for 2D visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a hard-margin SVM\n",
    "clf_hard = SVC(C=np.inf, kernel='linear')\n",
    "clf_hard.fit(X_train, y_train)\n",
    "\n",
    "# Train a soft-margin SVM\n",
    "clf_soft = SVC(C=1, kernel='linear')\n",
    "clf_soft.fit(X_train, y_train)\n",
    "\n",
    "# Plot decision boundaries\n",
    "def plot_decision_boundary(clf, X, y, title):\n",
    "    h = .02  # Step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(clf_hard, X_train, y_train, 'Hard Margin SVM')\n",
    "plot_decision_boundary(clf_soft, X_train, y_train, 'Soft Margin SVM')\n",
    "Q6. SVM Implementation through Iris Dataset\n",
    "Implementation Using Scikit-Learn:\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load and prepare the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Linear SVM\n",
    "svm = SVC(kernel='linear', C=1)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "Bonus Task: Implement a Linear SVM Classifier from Scratch\n",
    "From Scratch Implementation:\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class LinearSVM:\n",
    "    def __init__(self, C=1.0, lr=0.01, epochs=1000):\n",
    "        self.C = C\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        y = np.where(y == 0, -1, 1)  # Convert 0, 1 labels to -1, 1\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            for i in range(n_samples):\n",
    "                if y[i] * (np.dot(X[i], self.w) + self.b) < 1:\n",
    "                    self.w -= self.lr * (2 * self.C * self.w - np.dot(X[i], y[i]))\n",
    "                    self.b -= self.lr * y[i]\n",
    "                else:\n",
    "                    self.w -= self.lr * (2 * self.C * self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) + self.b)\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Using only the first two features\n",
    "y = iris.target\n",
    "\n",
    "# Binary classification for simplicity (class 0 vs. class 1)\n",
    "y = np.where(y == 2, 1, 0)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train and evaluate the LinearSVM model\n",
    "model = LinearSVM(C=1.0, lr=0.01, epochs=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Linear SVM Accuracy (from scratch): {accuracy:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
